### 2020-09-07

-----
##### 1. 학습 날짜
- 2020-09-07

-----
##### 2. 학습 시간
- 11:00 ~ 23:00

-----
###### 3. 학습 범위 및 주제
- Flask
- 자연어처리

-----
##### 4. 동료 학습 방법
- slack

-----
##### 5. 학습 목표
- 프로젝트 리팩토링

-----
##### 6. 과제 제출
- https://github.com/Gaepodong/automatic_tagging_review_ratings

-----
##### 7. 상세 학습 내용
- 프로젝트는 아직 끝난게 아니었다. ui적으로는 끝났지만 내부적으로 수정할 부분이 조금 남아있었따. 수정하는 김에 주석도 깔끔하게 새로 만들었다.
- 자연어처리
    - 자연어 처리의 역사적 흐름? 을 정리해봤다.
        - seq2seq2 모델은 입력으로 시퀀스가 들어온다. 이 시퀀스에는 자연어 문장, 음악 데이터, 또는 스트리밍 영상 데이터가 들어올 수 있다. 입력으로 들어온 이 시퀀스는 모델에 의해 다른 시퀀스가 되어서 나간다.
        - seq2seq를 처리하는 고전적인 방법은 RNN이다. RNN은 좋은 성능을 보여주었다. 하지만 RNN은 태생적인 문제가 있다. 바로 병목현상과 기울기 소실이다. 병목현상이 나타나는 이유는 하나 토큰을 처리하기 위해서는 이전 토큰의 히든 스테이트가 필요하다. 즉 이전 출력의 결과값이 현재 입력으로 들어와야 하기 때문에 데이터를 처리하는데 큰 시간이 든다. 또 이전의 출력이 이후의 입력으로 들어가게 만듦으로써 시퀀스의 각 토큰이 서로에게 영향을 미칠 수 있게 되었지만, 사이의 거리가 먼 토큰들은 서로에게 미치는 영향이 적게 된다. 이 두가지 문제를 해결하기 위해 LSTM, GRU 등이 등장했지만 역부족이었다.
        - 이후에 RNN을 보완하고자 나온 방법은 attention 기술이다. 이 attention 기술을 적용하면 현재 처리하는 토큰이 토큰 사이의 거리에 상관없이 영향을 받을 수 있었다. 하지만 RNN + attention도 결국 RNN을 베이스로 하는 모델이기 때문에 개선이 필요했다.
        - 결국 RNN의 기본 베이스를 전부 버린 Transformer이 등장했다.
        - Transformer은 ATTENTION 기술만을 이용해서 구현되었다. 따라서 RNN의 태생적인 한계를 벗어 날 수 있었다. 또한 성능또한 탁월했다.
        - 그리고 이제 GPT라는 모델이 등장했다. 이 모델은 Pre trained의 약자다. 이 모델이 왜 나왔냐면, 보통 딥러닝을 학습 할 때 라벨링이 된 데이터를 이용하는데 실제 세계에는 라벨링이 된 데이터는 거의 없고 대부분이 되지 않는 데이터다. 만약 우리가 라벨링이 되지 않은 데이터를 unsupervised한 방법으로 먼저 train하고 모델을 만든다음에 우리가 진정으로 처리해야하는 specific한 task에 적용을 해보면 되지 않을까 라는 물음에서 나왔다. 그리고 이 모델은 실제로 매우 좋은 성능을 보여줬다.


-----
##### 8. 오늘 학습 내용에 대한 개인적인 총평
- 많은 시간 앉아있었는데 막상 한건 없는거 같다.. 이러면 안된다. 집중하자..

-----

##### 9. 다음 학습 계획

- 웹
- 문제풀이
